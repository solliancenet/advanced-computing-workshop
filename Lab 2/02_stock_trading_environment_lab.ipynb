{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "!pip install gym_anytrading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670176756138
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import gym\n",
        "import gym_anytrading\n",
        "\n",
        "from stable_baselines3 import A2C\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Load and Review Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "df = gym_anytrading.datasets.STOCKS_GOOGL.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670177240526
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670177255376
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "df.tail(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670184456929
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "len(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Register the stock trading environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670186156246
        }
      },
      "outputs": [],
      "source": [
        "window_size = 10\n",
        "start_index = window_size\n",
        "end_index = len(df)\n",
        "\n",
        "env_maker = lambda: gym.make(\n",
        "    'stocks-v0',\n",
        "    df = df,\n",
        "    window_size = window_size,\n",
        "    frame_bound = (start_index, end_index)\n",
        ")\n",
        "\n",
        "env = DummyVecEnv([env_maker])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "The stock trading environment is a good example of a custom environment. A custom environment follows the OpenAI gym interface and at minimum needs to provide these methods:\n",
        "\n",
        "- `__init__()`: initializes the instance of the environment. The most important thing this method does is define the action space and the observation space.\n",
        "- `reset()`: resets the state of the environment and returns the initial observation.\n",
        "- `step(action)`: executes the specified action in the environment, returning the observation that results along with the immediate reward, a flag indicating if the episode is ended and additional information specific to the scenario.\n",
        "\n",
        "Let's explore some fo the highlights of the stocks-v0 environment. Make sure to follow the inline links to see the code being described in the source repo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## The base environment class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "The `gym_anytrading` library provides the `TradingEnv` base class that represents the general case for any form of trading environment. \n",
        "\n",
        "It takes the simplest approach of defining the [Actions](https://github.com/AminHP/gym-anytrading/blob/master/gym_anytrading/envs/trading_env.py#L9) that an agent can select from as simply either `Sell` or `Buy`. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Environment Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "In the `__init__` method of the `TradingEnv` class it defines the [action space](https://github.com/AminHP/gym-anytrading/blob/master/gym_anytrading/envs/trading_env.py#L36) as a discrete action space (e.g., the values are either 0 for sell or 1 for buy). It also defines the [observation space](https://github.com/AminHP/gym-anytrading/blob/master/gym_anytrading/envs/trading_env.py#L37) as continous, as a `Box` (e.g., the real values range between negative infinity and positive infinity).\n",
        "\n",
        "In other words, in this environment the agent has discrete options to buy or sell. The observation the environment returns after performing the action reflect the next price of the asset at the next tick in time of the dataset.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Environment Reset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "The [reset](https://github.com/AminHP/gym-anytrading/blob/master/gym_anytrading/envs/trading_env.py#L58) method does what you would expect, it resets the state of the environment (e.g., set the accumulated reward to 0, set the tick counter back to the start) and returns an observation that is basically the first price from the history of price data of the asset being traded.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Environment Step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "The [step](https://github.com/AminHP/gym-anytrading/blob/master/gym_anytrading/envs/trading_env.py#L71) method is where the action the agent selected affects the state of the environment. In the TradingBase class this involves addressing the following:\n",
        "\n",
        "- _Are we done?_ Compute if the action resulted in the episode being over by [setting the done flag](https://github.com/AminHP/gym-anytrading/blob/master/gym_anytrading/envs/trading_env.py#L75). In this case, the episode is over when we have reached the last tick in the range of ticks provided. \n",
        "- _What reward did the agent earn?_ The [immediate reward that results](https://github.com/AminHP/gym-anytrading/blob/master/gym_anytrading/envs/trading_env.py#L78) from the taking the action the agent requested is calculated. In the base class, this is just a stub function that is specialized by derived classes, such as that stock trading class we will cover shortly. \n",
        "- _Update environment internal state._ In this case the environment [updates its running total of the rewards earned](https://github.com/AminHP/gym-anytrading/blob/master/gym_anytrading/envs/trading_env.py#L79), the [profit so far](https://github.com/AminHP/gym-anytrading/blob/master/gym_anytrading/envs/trading_env.py#L81), and if a trade took place [toggling its position from a Short to a Long](https://github.com/AminHP/gym-anytrading/blob/master/gym_anytrading/envs/trading_env.py#L89) or vice-versa.\n",
        "- Return the [observation, immediate reward, done flag and any additional metadata](https://github.com/AminHP/gym-anytrading/blob/master/gym_anytrading/envs/trading_env.py#L101) that the environment can communicate to the agent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# The stocks-v0 environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "The `stocks-v0` environment we loaded previously is represented by the [StocksEnv class](https://github.com/AminHP/gym-anytrading/blob/master/gym_anytrading/envs/stocks_env.py#L6) which derives from the TradingEnv class.\n",
        "\n",
        "In the [init of the environment](https://github.com/AminHP/gym-anytrading/blob/master/gym_anytrading/envs/stocks_env.py#L8), it configures a default fee amount for bid and ask trades. These fees are expressed in terms of units (e.g., a bid fee is 1% of one share).\n",
        "\n",
        "The [immediate reward is calculated](https://github.com/AminHP/gym-anytrading/blob/master/gym_anytrading/envs/stocks_env.py#L41) if a trade took place. It earns a reward that is the difference in price between the current price and when the agent last made a trade. The reward provided to the agent does not incorporate buy or sell fees.\n",
        "\n",
        "The reward is calculated as follows:\n",
        "- Positive reward: An agent earns a positive reward when it sells a stock at a higher price than what it had bought it at previously. \n",
        "- Negative reward: It might earn a negative reward if it sold below the price at which it bought. \n",
        "- Zero reward: If the agent did not make a trade, or if the trade placed was to buy the stock. In other words, the agent only gets a reward when it sells something it had previously bought."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Train the trading agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670186905626
        },
        "jupyter": {
          "outputs_hidden": true
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "policy_kwargs = dict(net_arch=[dict(vf=[128, 128, 128], pi=[64, 64])])\n",
        "model = A2C('MlpPolicy', env, verbose=1, policy_kwargs=policy_kwargs)\n",
        "model.learn(total_timesteps=1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluate the agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670186907345
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "env = env_maker()\n",
        "observation = env.reset()\n",
        "\n",
        "while True:\n",
        "    observation = observation[np.newaxis, ...]\n",
        "    action, _states = model.predict(observation)\n",
        "    observation, reward, done, info = env.step(action)\n",
        "\n",
        "    #env.render()\n",
        "    if done:\n",
        "        print(\"info:\", info)\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670186301573
        }
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(16, 6))\n",
        "env.render_all()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Examine the quantative results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "!pip install quantstats --upgrade --no-cache-dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670186977468
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "import quantstats as qs\n",
        "qs.extend_pandas()\n",
        "\n",
        "net_worth = pd.Series(env.history['total_profit'], index=df.index[start_index+1:end_index])\n",
        "returns = net_worth.pct_change().iloc[1:]\n",
        "\n",
        "qs.reports.full(returns)\n",
        "qs.reports.html(returns, output='a2c_quantstats.html')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "So it's not a great agent (there are lot of things that you could do to improve its performance), but that was not the point- the goal was to learn how to build an environment for a novel scenario."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "p3.ipynb",
      "provenance": []
    },
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3.10.3 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.3"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "1abc2c795f1f239c96feeea9f47a6887dba211b6165f2513ef16d26ebe9b029f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

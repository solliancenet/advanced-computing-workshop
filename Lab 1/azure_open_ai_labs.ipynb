{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": true,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "!pip install openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the following cell to load the Azure Open AI API key from the Key Vault pre-configured in your environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670484674370
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azureml.core import Workspace\n",
        "ws = Workspace.from_config()\n",
        "keyvault = ws.get_default_keyvault()\n",
        "open_ai_api_key = keyvault.get_secret(name=\"open_ai_api_key\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Summarization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the following cell to initialize the setting use the Open AI API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670102829479
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "openai.api_type = \"azure\"\n",
        "openai.api_base = \"https://solliance-openai-01.openai.azure.com/\"\n",
        "openai.api_version = \"2022-06-01-preview\"\n",
        "openai.api_key = open_ai_api_key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Edit the value of the prompt parameter with the text you would like to summarize and the run the following cell to see the summarization produced."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670115817303
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "response = openai.Completion.create(\n",
        "  engine=\"gpt3-text-ada-001\",\n",
        "  prompt=\"A neutron star is the collapsed core of a massive supergiant star, which had a total mass of between 10 and 25 solar masses, possibly more if the star was especially metal-rich.[1] Neutron stars are the smallest and densest stellar objects, excluding black holes and hypothetical white holes, quark stars, and strange stars.[2] Neutron stars have a radius on the order of 10 kilometres (6.2 mi) and a mass of about 1.4 solar masses.[3] They result from the supernova explosion of a massive star, combined with gravitational collapse, that compresses the core past white dwarf star density to that of atomic nuclei.\\n\\nTl;dr\",\n",
        "  temperature=0.7,\n",
        "  max_tokens=60,\n",
        "  top_p=1,\n",
        "  frequency_penalty=0,\n",
        "  presence_penalty=0,\n",
        "  stop=None\n",
        ")\n",
        "\n",
        "response.choices[0].text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Introducing Embeddings\n",
        "\n",
        "In the following try entering any text as input (be careful to exclude new line characters) and observe the embedding that results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670108343691
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "response = openai.Embedding.create(\n",
        "  engine=\"gpt3-similarity-babbage-001\",\n",
        "  input=\"A neutron star is the collapsed core of a massive supergiant star.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670108531628
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "response.data[0].embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Using Embeddings to cluster \"similar\" text and derive insights\n",
        "\n",
        "The following cells download a sample dataset that has already had the embeddings processed with the `text-similarity-babbage` model, similar to how you called it in the previous cell.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670105421361
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# for convenience, we precomputed the embeddings\n",
        "datafile_path = \"https://cdn.openai.com/API/examples/data/fine_food_reviews_with_embeddings_1k.csv\"  \n",
        "df = pd.read_csv(datafile_path)\n",
        "df[\"babbage_similarity\"] = df.babbage_similarity.apply(eval).apply(np.array)\n",
        "matrix = np.vstack(df.babbage_similarity.values)\n",
        "matrix.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670105462830
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670105519380
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "n_clusters = 4\n",
        "\n",
        "kmeans = KMeans(n_clusters=n_clusters, init=\"k-means++\", random_state=42)\n",
        "kmeans.fit(matrix)\n",
        "labels = kmeans.labels_\n",
        "df[\"Cluster\"] = labels\n",
        "\n",
        "df.groupby(\"Cluster\").Score.mean().sort_values()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the following cells you will use a dimensionality reduction technique that reduces the number of dimensions of the data down to two dimensions that can then be visualized easily in a scatter plot. Run the cell to see the clusters identified."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670107438079
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tsne = TSNE(\n",
        "    n_components=2, perplexity=15, random_state=42, init=\"random\", learning_rate=200\n",
        ")\n",
        "vis_dims2 = tsne.fit_transform(matrix)\n",
        "\n",
        "x = [x for x, y in vis_dims2]\n",
        "y = [y for x, y in vis_dims2]\n",
        "\n",
        "for category, color in enumerate([\"purple\", \"green\", \"red\", \"blue\"]):\n",
        "    xs = np.array(x)[df.Cluster == category]\n",
        "    ys = np.array(y)[df.Cluster == category]\n",
        "    plt.scatter(xs, ys, color=color, alpha=0.3)\n",
        "\n",
        "    avg_x = xs.mean()\n",
        "    avg_y = ys.mean()\n",
        "\n",
        "    plt.scatter(avg_x, avg_y, marker=\"x\", color=color, s=100)\n",
        "plt.title(\"Clusters identified visualized in language 2d using t-SNE\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the following cell to see a sample of a review from each cluster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670107533350
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Reading a review which belong to each group.\n",
        "rev_per_cluster = 3\n",
        "\n",
        "for i in range(n_clusters):\n",
        "    print(f\"Cluster {i} Theme:\", end=\" \")\n",
        "\n",
        "    reviews = \"\\n\".join(\n",
        "        df[df.Cluster == i]\n",
        "        .combined.str.replace(\"Title: \", \"\")\n",
        "        .str.replace(\"\\n\\nContent: \", \":  \")\n",
        "        .sample(rev_per_cluster, random_state=42)\n",
        "        .values\n",
        "    )\n",
        "    response = openai.Completion.create(\n",
        "        engine=\"gpt3-text-davinci-002\",\n",
        "        prompt=f'What do the following customer reviews have in common?\\n\\nCustomer reviews:\\n\"\"\"\\n{reviews}\\n\"\"\"\\n\\nTheme:',\n",
        "        temperature=0,\n",
        "        max_tokens=64,\n",
        "        top_p=1,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0,\n",
        "        stop=None\n",
        "    )\n",
        "    print(response[\"choices\"][0][\"text\"].replace(\"\\n\", \"\"))\n",
        "\n",
        "    sample_cluster_rows = df[df.Cluster == i].sample(rev_per_cluster, random_state=42)\n",
        "    for j in range(rev_per_cluster):\n",
        "        print(sample_cluster_rows.Score.values[j], end=\", \")\n",
        "        print(sample_cluster_rows.Summary.values[j], end=\":   \")\n",
        "        print(sample_cluster_rows.Text.str[:70].values[j])\n",
        "\n",
        "    print(\"-\" * 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Searching with embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Get the embedding for the search text. It's important to understand that for search, we use a different approach to computing the embeddings and resolving matches. \n",
        "\n",
        "- Query: For our search text, we need to use a *query* model (e.g., `text-search-babbage-query-001`)\n",
        "- Documents: For the documents we want to search against, these embeddings need to be calculated with a *doc* model (e.g., `text-search-babbage-doc-001`)\n",
        "\n",
        "These are two different engines in the parlance of Azure OpenAI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670111344609
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from openai.embeddings_utils import get_embedding, cosine_similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Let's calculate the document embeddings for our corpus of documents that we can search against.\n",
        "\n",
        "We'll start with getting the embedding for one document from our corpus so we can examine its shape. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670112207709
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "response = openai.Embedding.create(\n",
        "  engine=\"gpt3-search-babbage-doc-001\",\n",
        "  input=df.head(1)['Text'][0]\n",
        ")\n",
        "\n",
        "doc_text_embedding = response.data[0].embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "The embeddding of the doc has the following dimensions (run the cell to find out):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670112257376
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "len(doc_text_embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "So the document is embedded with 2,048 dimensions when using Babbage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Now, get the embedding for each document in our corpus and store that value in the `babbage_search_2` field that we add to the dataframe.\n",
        "\n",
        "NOTE: This will take about 2-3 minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670110983291
        },
        "jupyter": {
          "outputs_hidden": true,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "df[\"babbage_search_2\"] = df.Text.apply(lambda x: get_embedding(x, engine=\"gpt3-search-babbage-doc-001\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Next, recall that when you search you have to get the embedding of your search text using the `query` model. \n",
        "\n",
        "Let's start by getting the embedding for a search query and examine how many dimensions it has."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670113205775
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "response = openai.Embedding.create(\n",
        "  engine=\"gpt3-search-babbage-query-001\",\n",
        "  input=\"great beans\" #try other inputs like \"bad taste\", \"spoilt\" or \"pet food\"\n",
        ")\n",
        "\n",
        "search_text_embedding = response.data[0].embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "As before, the length of the array tells us the number of dimensions used by this embedding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670112488949
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "len(search_text_embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Did you notice something interesting in comparing the number of dimensions between the query and the doc? \n",
        "\n",
        "Yes, that's right! They have the same dimensions, even though one is a short text string and another is a long document. In fact, for search to work, they MUST have the same dimensions. \n",
        "\n",
        "NOTE: This explains why we couldn't use the pre-computed embedding in the `babbage_search` field of the sample data file. If you run the following cell you'll see that this one has dimensions that do not match the query. If you were to try to compute the cosine distance with these, you would get an error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670110416759
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "len(df.head(1)['babbage_search'][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Ok, now we get to the fun part. Let's compute just how close our query is to the first doc in the sample data set. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670113214888
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "cosine_similarity(df.head(1)['babbage_search_2'][0], search_text_embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "The `cosine_similarity` function returns a value between `0.0` and `1.0` where the closer the value is to `1.0` the more similar the two.\n",
        "\n",
        "Run the next cell to compute the similarity between the query and the doc, for each doc in the dataset. We'll sort the data by the similarities score so that higher scores appear first and then we can pick the top N to see the best results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670113219739
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "df[\"similarities\"] = df.babbage_search_2.apply(lambda x: cosine_similarity(x, search_text_embedding))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Lets take a look at the top 3 closest results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670113221853
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_colwidth', 320)\n",
        "pd.set_option('display.max_rows', None)\n",
        "\n",
        "df.sort_values(\"similarities\", ascending=False).head(n=3)[['Text', 'similarities']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Do the above results appear like a good match for your search query?"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3.10.3 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.3"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "1abc2c795f1f239c96feeea9f47a6887dba211b6165f2513ef16d26ebe9b029f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
